{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e245b028",
   "metadata": {},
   "source": [
    "# Customer Churn prediction\n",
    "\n",
    "## 1. Define the Business Problem and ML Solution\n",
    "\n",
    "First step is to define your problem statement, how to solve it and what to evaluate it.\n",
    "\n",
    "## 2. Explore the Data\n",
    "In this section, we explore and analyze the data to gain insights and understanding.\n",
    "\n",
    "## 3. Data Preparation\n",
    "This section covers the preprocessing and cleaning steps performed on the data to make it suitable for training the models.\n",
    "\n",
    "## 4. Training & Evaluation\n",
    "Here, we train the models using the prepared data and evaluate their performances.\n",
    "\n",
    "## 5. Conclusions\n",
    "Finally, we summarize the key findings and conclusions from the analysis and training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a03dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa012255",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report,confusion_matrix , roc_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\n",
    "\n",
    "\n",
    "## ML Models Diffrent Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c994c3e",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_path='E_Commerce_Dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7915de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pandas read_csv\n",
    "#For example: df=pd.read_csv(....)\n",
    "#For more https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html\n",
    "\n",
    "#your code starts here\n",
    "\n",
    "#your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f33e288",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To visualize the data, run 'head()' function\n",
    "# For more https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html\n",
    "\n",
    "#your code starts here\n",
    "\n",
    "#your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc4456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that there are few NaNs in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d14d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to explore the data further using shape and describe() methods\n",
    "#For more on 'shape': https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html \n",
    "#For more on 'describe()': https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49293b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print shape of data\n",
    "\n",
    "#your code starts here\n",
    "\n",
    "#your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d77c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print description of data\n",
    "print(\"\\nNumerical  Features\")\n",
    "\n",
    "#your code starts here\n",
    "\n",
    "#your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a297fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notice how the describe function prints numerical features only.\n",
    "#this behavior can be changed by modifying the 'include' parameter like this: df.describe(include='object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2dfcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCategorical Features\")\n",
    "\n",
    "#your code starts here\n",
    "\n",
    "#your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107cfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values df.isna()\n",
    "#Followed by .sum() to check number of Nulls per feature\n",
    "print(\"--------------------Data NA Check-------------------------------\")\n",
    "\n",
    "#your code starts here\n",
    "\n",
    "#your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6681f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable (label)\n",
    "sns.countplot(x=\"Churn\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069086c",
   "metadata": {},
   "source": [
    "## TimeCheck1\n",
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5180db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's Balance the Dataset\n",
    "# Separate majority and minority classes\n",
    "majority_samples = df[df['Churn'] == 0]\n",
    "minority_samples = df[df['Churn'] == 1]\n",
    "\n",
    "# Downsample the majority class\n",
    "downsampled_majority = resample(majority_samples,\n",
    "                                replace=False,  # Set to False for downsampling\n",
    "                                n_samples=2*len(minority_samples),  # Match minority class size\n",
    "                                random_state=42)  # For reproducibility\n",
    "\n",
    "# Combine the downsampled majority class with the minority class\n",
    "balanced_dataset = pd.concat([downsampled_majority, minority_samples])\n",
    "df=balanced_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-check the distribution of the target variable (label)\n",
    "sns.countplot(x=\"Churn\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d8e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop Null values. To do that, we can drop all rows that have null values\n",
    "# You can use 'df.dropna' \n",
    "# Make sure to specify axis=0 to remove rows with null values and inplace=True to modify the df\n",
    "# for more: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html\n",
    "\n",
    "#your code starts here\n",
    "\n",
    "#your code ends here\n",
    "\n",
    "print(\"-----------------After removing the null values-----------------\")\n",
    "print(df.isna().sum())\n",
    "print(\"-----------------shape of data after removing nulls--------------\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation between the variables\n",
    "fig, ax = plt.subplots(figsize=(20,10)) \n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93f84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to preprocess the data before we start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239a7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at the data type of each feature\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacf766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that some features are of type object and ML models expect numerical inputs\n",
    "#Hence, we will need to convert the categorical variables into numerical \n",
    "#We can use LabelEncoder as follows:\n",
    "le = LabelEncoder()\n",
    "df['PreferredLoginDevice'] = le.fit_transform(df['PreferredLoginDevice'])\n",
    "\n",
    "#Apply the same on other features that are of type objects\n",
    "\n",
    "#your code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#your code ends here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea602e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at the data type of each feature after applying LabelEncoder\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cdac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's have a look at one of the categorical featuers \n",
    "df.Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f0edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop meaningless features and divide data into X and y. \n",
    "X = df.drop(['CustomerID', 'Churn'], axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "# Split the dataset into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69786ee8",
   "metadata": {},
   "source": [
    "## Timecheck2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e5256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a DecisionTreeClassifier and RandomForestClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67815c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize = (4,2))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_dt),fmt='g',annot=True)\n",
    "plt.ylabel('Actual',fontsize=13)\n",
    "plt.xlabel('Predicted',fontsize=13)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944956ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarly, train a 'RandomForestClassifier()'\n",
    "#Print classification report and draw Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a RandomForestClassifier and print classification_report\n",
    "#Your code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#your code ends here\n",
    "\n",
    "# Plot the confusion matrix\n",
    "#Your code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38a8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is a mehtod to calculate the feature importances \n",
    "feature_importance = rf.feature_importances_\n",
    "feature_names = X_test.columns\n",
    "sorted_indices = np.argsort(feature_importance)[::-1]  # Sort indices in descending order\n",
    "for idx in sorted_indices:\n",
    "    print(f\"{feature_names[idx]}: {feature_importance[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972983da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's test a sample, you can try any values and check the prediciton\n",
    "test_sample = [[\n",
    "      1, #'Tenure'\n",
    "      1, #PreferredLoginDevice\n",
    "      1, #CityTier\n",
    "      14,#WarehouseToHome\n",
    "      1, #PreferredPaymentMode\n",
    "      1, #Gender\n",
    "      4, #HourSpendOnApp\n",
    "      4, #NumberOfDeviceRegistered\n",
    "      4, #PreferedOrderCat\n",
    "      1, #SatisfactionScore\n",
    "      1, #MaritalStatus\n",
    "      2, #NumberOfAddress\n",
    "      1, #Complain\n",
    "      12,#OrderAmountHikeFromlastYear\n",
    "      7, #CouponUsed\n",
    "      10,#OrderCount\n",
    "      9, #DaySinceLastOrder\n",
    "      200#CashbackAmount\n",
    "     ]]\n",
    "print('Sample Churn prediction: ',rf.predict(test_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230db04",
   "metadata": {},
   "source": [
    "## The END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363e178c",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "![Confusion_Matrix](https://www.researchgate.net/publication/326866871/figure/fig3/AS:669601385959430@1536656819610/22-confusion-matrix-and-associated-measures.ppm)\n",
    "\n",
    "### Accuracy\n",
    "    Accuracy is the most basic evaluation metric, representing the The measure of how many observations our model correctly predicted.\n",
    "    Pros:\n",
    "        Easy to understand and interpret.\n",
    "        Suitable for balanced datasets.\n",
    "    Cons:\n",
    "        Can be misleading when dealing with imbalanced datasets.\n",
    "        Ignores the types of errors (false positives and false negatives).\n",
    "    Example: \n",
    "        In a binary classification task, if a model makes 80 correct predictions out of 100, the accuracy would be 80%.\n",
    "\n",
    "### Precision\n",
    "    Precision measures how correct our 1’s are.\n",
    "    Pros:\n",
    "        Useful when the focus is on minimizing false positives.\n",
    "        Provides insights into the model's ability to avoid false alarms.\n",
    "    Cons:\n",
    "        Does not consider false negatives.\n",
    "    Example: \n",
    "        Abuse detection - In order to avoid accusing good users, we need to make sure a user is actually an abuser when flagged as 1.\n",
    "\n",
    "### Recall\n",
    "    Recall, also known as sensitivity or true positive rate, measures how many 1’s we might have missed out of all actual 1's.\n",
    "    Pros:\n",
    "        Important when the focus is on minimizing false negatives.\n",
    "        Indicates the model's ability to identify positive cases.\n",
    "    Cons:\n",
    "        Does not account for false positives.\n",
    "    Example: \n",
    "        Running a marketing campaign with unlimited budget and we want to send coupons to users that are at high risk of leaving so that we encourage them to stay - It’s important to capture as many 1’s as possible.\n",
    "\n",
    "### F1 Score\n",
    "    The F1 score is the balance between precision and recall and an evaluation metric that considers both aspects.\n",
    "    Pros:\n",
    "        Takes into account both false positives and false negatives.\n",
    "        Suitable for imbalanced datasets.\n",
    "    Cons:\n",
    "        F1 score gives equal weight to precision and recall, which may not always be desired.\n",
    "    Example: \n",
    "        Similar to previous example, but now we have limited marketing budget and can’t give out unlimited coupons. In this case, if only recall is high but precision is low then we might be spending a lot of money on users that may or may not have been high-risk, but if both precision and recall are high (which also means f1 score is high) then it’s a good balance between identifying potential high-risk users without spending a lot of money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7114050e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
